{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook analyzes a toughies.csv file. This file is generated by the Django management command named `get_toughie_info.py` with a production db backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import random\n",
    "import csv\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the following variables to CSW or NWL depending on what we are generating (and what the input is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEXICAL_FAMILY = 'CSW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toughies = pandas.read_csv('./toughies.csv')\n",
    "len(toughies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For better statistical significance, filter only bingos that were asked at least 30 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_toughies = toughies.loc[toughies['asked'] >= 26]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order of dictionary updates:\n",
    "def lexkey_assigner(row):\n",
    "    return {\n",
    "        'OWL2': 1,\n",
    "        'America': 2,\n",
    "        'NWL18': 3,\n",
    "        'NWL20': 4,\n",
    "        ## CSW\n",
    "        'CSW12': 5,\n",
    "        'CSW15': 6,\n",
    "        'CSW19': 7,\n",
    "    }[row['lexicon']]\n",
    "\n",
    "better_toughies = better_toughies.assign(\n",
    "    lexkey=better_toughies.apply(lexkey_assigner, axis=1)).sort_values('lexkey')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which words have NOT been asked yet. \n",
    "# Can create these files like this, for example:\n",
    "# sqlite> .mode csv\n",
    "# sqlite> .output 7s_alphs.csv\n",
    "# sqlite> select alphagram, contains_word_uniq_to_lex_split from alphagrams where length = 7;\n",
    "# sqlite> .output 8s_alphs.csv\n",
    "# sqlite> select alphagram, contains_word_uniq_to_lex_split from alphagrams where length = 8;\n",
    "\n",
    "if LEXICAL_FAMILY == 'CSW':\n",
    "    lexica = ['CSW12', 'CSW15', 'CSW19']\n",
    "elif LEXICAL_FAMILY == 'NWL':\n",
    "    lexica = ['OWL2', 'America', 'NWL18', 'NWL20']\n",
    "\n",
    "LATEST_LEXICON = lexica[-1]\n",
    "\n",
    "alphas_7s = pandas.read_csv(f'./{LATEST_LEXICON}_7s_alphs.csv', header=None, names=['Alphagram', 'lexuniq'], index_col='Alphagram')\n",
    "alphas_8s = pandas.read_csv(f'./{LATEST_LEXICON}_8s_alphs.csv', header=None, names=['Alphagram', 'lexuniq'], index_col='Alphagram')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_7s.loc['ACEIORT']['lexuniq']\n",
    "# alphas_8s.loc['AEINOSTV']['lexuniq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These text files below were created like:\n",
    "# sqlite> .header on\n",
    "# sqlite> .output 7s_newin_CSW19.csv\n",
    "# sqlite> select alphagram from alphagrams where length(alphagram) = 7 and contains_update_to_lex = 1;\n",
    "# sqlite> .output 8s_newin_CSW19.csv\n",
    "# sqlite> select alphagram from alphagrams where length(alphagram) = 8 and contains_update_to_lex = 1;\n",
    "\n",
    "\n",
    "\n",
    "new_sevens_first_update = pandas.read_csv(f'./7s_newin_{lexica[1]}.csv', index_col='alphagram')\n",
    "new_sevens_second_update = pandas.read_csv(f'./7s_newin_{lexica[2]}.csv', index_col='alphagram')\n",
    "new_eights_first_update = pandas.read_csv(f'./8s_newin_{lexica[1]}.csv', index_col='alphagram')\n",
    "new_eights_second_update = pandas.read_csv(f'./8s_newin_{lexica[2]}.csv', index_col='alphagram')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "better_toughies.loc[better_toughies['Alphagram'] == 'AADHNPR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_toughies.loc[better_toughies['Alphagram'] == 'EEIRSTU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asked_7s = set()\n",
    "asked_8s = set()\n",
    "\n",
    "# Start at the first lexicon. Toughies are sorted chronologically by lexicon.\n",
    "last_lex = lexica[0]\n",
    "for _, row in better_toughies.iterrows():\n",
    "    lex = row['lexicon']\n",
    "    alpha = row['Alphagram']\n",
    "    if lex not in lexica:\n",
    "        continue\n",
    "    # Clear out questions that got new additions.\n",
    "    if lex != last_lex:\n",
    "        if lex == lexica[1]:\n",
    "            for row in new_sevens_first_update.iterrows():\n",
    "                if row[0] in asked_7s:\n",
    "                    asked_7s.remove(row[0])\n",
    "            for row in new_eights_first_update.iterrows():\n",
    "                if row[0] in asked_8s:\n",
    "                    asked_8s.remove(row[0])\n",
    "        elif lex == lexica[2]:\n",
    "            for row in new_sevens_second_update.iterrows():\n",
    "                if row[0] in asked_7s:\n",
    "                    asked_7s.remove(row[0])\n",
    "            for row in new_eights_second_update.iterrows():\n",
    "                if row[0] in asked_8s:\n",
    "                    asked_8s.remove(row[0])\n",
    "        \n",
    "    if len(alpha) == 7 and alpha in alphas_7s.index:\n",
    "        asked_7s.add(alpha)\n",
    "    if len(alpha) == 8 and alpha in alphas_8s.index:\n",
    "        asked_8s.add(alpha) \n",
    "        \n",
    "    last_lex = lex\n",
    "        \n",
    "print(f'Asked {len(asked_7s)} out of {len(alphas_7s)} 7s')\n",
    "print(f'Asked {len(asked_8s)} out of {len(alphas_8s)} 8s')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((len(alphas_7s) - len(asked_7s)) / 50)\n",
    "print((len(alphas_8s) - len(asked_8s)) / 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to add asked_7s and asked_8s from NWL as long as the alphagram doesn't have any CSW-only words.\n",
    "# i.e. imagine OUTRIDE has not been asked in CSW. We should not add the stats from NWL because ETOURDI# / IODURET# \n",
    "if LEXICAL_FAMILY == 'CSW':\n",
    "    lexica = ['OWL2', 'America', 'NWL18', 'NWL20']\n",
    "    for _, row in better_toughies.iterrows():\n",
    "        lex = row['lexicon']\n",
    "        if lex not in lexica:\n",
    "            continue    \n",
    "        alpha = row['Alphagram']\n",
    "        if len(alpha) == 7:\n",
    "            if alpha not in alphas_7s.index:\n",
    "                # Skip the very few NWL-only bingos\n",
    "                continue\n",
    "            if alphas_7s.loc[alpha]['lexuniq']:\n",
    "                # Skip if it has been asked in NWL, but the alphagram has a CSW-only solution\n",
    "                continue\n",
    "            asked_7s.add(alpha)\n",
    "        if len(alpha) == 8:\n",
    "            if alpha not in alphas_8s.index:\n",
    "                # Skip the very few NWL-only bingos\n",
    "                continue\n",
    "            if alphas_8s.loc[alpha]['lexuniq']:\n",
    "                continue\n",
    "\n",
    "            asked_8s.add(alpha)\n",
    "    \n",
    "    print(f'Counting NWL, asked {len(asked_7s)} out of {len(alphas_7s)} 7s')\n",
    "    print(f'Counting NWL, asked {len(asked_8s)} out of {len(alphas_8s)} 8s')\n",
    "    print((len(alphas_7s) - len(asked_7s)) / 50)\n",
    "    print((len(alphas_8s) - len(asked_8s)) / 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('missing 7s')\n",
    "print(len(set(alphas_7s.index) - asked_7s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('missing 8s')\n",
    "print(len(set(alphas_8s.index) - asked_8s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine a list of all bingos by difficulty!\n",
    "\n",
    "The methodology here is tricky. We can try a few things:\n",
    "\n",
    "1. Since the df is sorted from oldest to newest lexicon, results from newer \"asks\" will supersede older asks. We can make it so that if the new bingo was asked at least X more times (7 seems like an ok number, so if the old bingo was asked 70 times and the new bingo was asked 77 times, more data is better, so take the newer number), then we default to the newer number.\n",
    "\n",
    "2. The problem with 1 is that old Aerolith users skewed significantly more expert than new ones. Look up any tough bingo in the dataframe and the number wrong invariably goes up. So some very tough alphagrams, like ABGHOSTU, have only a 62.6% miss rate in the OWL2 days, but a 89.5% miss rate in the NWL18 days. We could strictly overwrite older data with newer data, but there is typically less newer data than there is older data, so the data might not be as good.\n",
    "\n",
    "3. Straight average. Since there is more old data than new data, this will still skew it a bit towards old data, but it might not be as bad.\n",
    "\n",
    "4. Weighted average. We can normalize the eras for the different lexica to the same toal number, or somehow count older data less than newer data.\n",
    "\n",
    "    e.g.\n",
    "    \n",
    "        Alphagram    probability    asked    missed    difficulty    lexicon \n",
    "        ABGHOSTU       15394        147      92        0.625850      OWL2    \n",
    "        ABGHOSTU       16059        77       61        0.792208      America \n",
    "        ABGHOSTU       16088        57       51        0.894737      NWL18   \n",
    "            \n",
    "    weigh it 20/30/50\n",
    "    0.2 * 0.6285 + 0.3 * 0.7922 + 0.5 * 0.894737 = 0.8107\n",
    "    or by raw numbers\n",
    "    (0.2 * 92 + 0.3 * 61 + 0.5 * 51) / (0.2 * 147 + 0.3 * 77 + 0.5 * 57) = 0.7679\n",
    "    \n",
    "    If we use NWL20, we can use weights: 0.15, 0.25, 0.3, 0.3 (for example)\n",
    "    (0.15 * 92 + 0.25 * 61 + 0.3 * 51) / (0.15 * 147 + 0.25 * 77 + 0.3 * 57) = 0.759"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bingos = {}\n",
    "# These numbers are somewhat hand-wavy. They should still result in decent results in most cases.\n",
    "weights = {\n",
    "    'OWL2': 0.15,\n",
    "    'America': 0.25,\n",
    "    'NWL18': 0.3,\n",
    "    'NWL20': 0.3,\n",
    "    # CSW numbers. These might or might not combine with the above. \n",
    "    'CSW12': 0.15,\n",
    "    'CSW15': 0.35,\n",
    "    'CSW19': 0.5,\n",
    "}\n",
    "pandas.set_option(\"display.max_rows\", 100, \"display.max_columns\", None)\n",
    "def aggregation_fn(group):\n",
    "    d = {}\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    probability = 0\n",
    "    for _, row in group.iterrows():\n",
    "        lex = row['lexicon']\n",
    "        if LEXICAL_FAMILY == 'CSW' and lex in ['OWL2', 'America', 'NWL18', 'NWL20']:\n",
    "            # Don't count NWL weights where the alphagram has CSW-only solutions,\n",
    "            # otherwise it's inaccurate:\n",
    "            if len(row['Alphagram']) == 8 and alphas_8s.loc[row['Alphagram']]['lexuniq']:\n",
    "                continue\n",
    "            if len(row['Alphagram']) == 7 and alphas_7s.loc[row['Alphagram']]['lexuniq']:\n",
    "                continue\n",
    "        numerator += row['missed'] * weights[row['lexicon']]\n",
    "        denominator += row['asked'] * weights[row['lexicon']]\n",
    "    d['weighted_difficulty'] = numerator/denominator\n",
    "    d['weighted_numerator'] = numerator\n",
    "    d['weighted_denominator'] = denominator\n",
    "    d['length'] = len(row['Alphagram'])\n",
    "    return pandas.Series(d)\n",
    "\n",
    "bingos = better_toughies.groupby('Alphagram').apply(aggregation_fn).reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.set_option('display.max_rows', 100)\n",
    "bingos.sort_values('weighted_difficulty', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bdf = pandas.DataFrame.from_dict(bingos, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Now we can ask some questions. For example, what are the hardest 1000 bingos with probability < 15000?\n",
    "\n",
    "# total = 100\n",
    "# prob_limit = 50000\n",
    "# alphas = bdf[bdf['probability'] <= prob_limit].sort_values('difficulty', ascending=False)[:total][['Alphagram', 'probability','difficulty']]\n",
    "# ct = 0\n",
    "# for _, row in alphas.iterrows():\n",
    "#     if len(row['Alphagram']) == 7:\n",
    "#         ct += 1\n",
    "# print(f'There are {ct} 7s out of {total}')\n",
    "# # for alpha in alphas:\n",
    "# #     print(alpha)\n",
    "# pandas.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "\n",
    "# print(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sevens = bingos[bingos[\"length\"] == 7].copy()\n",
    "eights = bingos[bingos[\"length\"] == 8].copy()\n",
    "nquantiles = 100\n",
    "\n",
    "qlabels = [f'q{q}' for q in range(nquantiles)]\n",
    "\n",
    "sevens.sort_values('weighted_difficulty', ascending=False)\n",
    "sevens['quantile'] = pandas.qcut(sevens['weighted_difficulty'], nquantiles, labels=qlabels)\n",
    "sevens.to_csv(f'{LATEST_LEXICON}_7s_difficulty.csv', index=True)\n",
    "print(pandas.qcut(sevens['weighted_difficulty'], nquantiles))\n",
    "\n",
    "eights.sort_values('weighted_difficulty', ascending=False)\n",
    "eights['quantile'] = pandas.qcut(eights['weighted_difficulty'], nquantiles, labels=qlabels)\n",
    "eights.to_csv(f'{LATEST_LEXICON}_8s_difficulty.csv', index=True)\n",
    "print(pandas.qcut(eights['weighted_difficulty'], nquantiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(sevens['weighted_difficulty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(eights['weighted_difficulty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bingos.loc[bingos['Alphagram'] == 'ABGHOSTU']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the following cell to determine which questions are left to ask (maybe can use for future updates or CSW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "missing_8s = list(set(alphas_8s.index) - asked_8s)\n",
    "missing_7s = list(set(alphas_7s.index) - asked_7s)\n",
    "\n",
    "asked_7s_new = list(asked_7s)\n",
    "asked_8s_new = list(asked_8s)\n",
    "random.shuffle(asked_7s_new)\n",
    "random.shuffle(asked_8s_new)\n",
    "\n",
    "# Extend the 8s by so we have 1500 exactly.\n",
    "missing_8s.extend(asked_8s_new[:371])\n",
    "# Extend the 7s so we have 200 exactly\n",
    "missing_7s.extend(asked_7s_new[:110])\n",
    "assert(len(missing_7s) == 200)\n",
    "assert(len(missing_8s) == 1500)\n",
    "\n",
    "random.shuffle(missing_7s)\n",
    "random.shuffle(missing_8s)\n",
    "\n",
    "i = 0\n",
    "for seven in missing_7s:\n",
    "    print(seven)\n",
    "    i += 1\n",
    "    if i % 50 == 0:\n",
    "        print('-' * 6)\n",
    "\n",
    "print ('-' * 12)\n",
    "i = 0\n",
    "for eight in missing_8s:\n",
    "    print(eight)\n",
    "    i += 1\n",
    "    if i % 50 == 0:\n",
    "        print('-' * 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge simulator - how many unasked questions do we have after a certain time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_days = 10 * 365   # We've been asking qs for roughly 10 years (since Jun 2011 -- update if changes)\n",
    "num_qs = 50\n",
    "num_alphas = 36549   # How many sevens or eights\n",
    "\n",
    "alphas = set(range(num_alphas))\n",
    "\n",
    "for i in range(num_days):\n",
    "    todays = list(range(num_alphas))\n",
    "    random.shuffle(todays)\n",
    "    for q in todays[:num_qs]:\n",
    "        if q in alphas:\n",
    "            alphas.remove(q)\n",
    "\n",
    "print(len(alphas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
